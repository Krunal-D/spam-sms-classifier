{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bab81-j-1c-3",
    "outputId": "d402f159-dc52-41ea-eaae-e153fc4dd957"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0QvPOJlw1uE2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2s83aB1q1_8j"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Spam SMS Collection', sep='\\t', names=['label', 'message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sOFVfXV03cxc",
    "outputId": "9cafd93e-298e-4f8a-a67c-8528f0a4df7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z6Ge57g22KqT",
    "outputId": "41624fb1-3da5-4479-a5fe-09c44ad0ebfe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Krunal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data Cleaning and Preprocessing\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yEo7GyV-2vsr"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "corpus = []\n",
    "for i in range(0, len(df)):\n",
    "  # Cleaning special character from the message\n",
    "  message = re.sub(pattern='[^a-zA-Z]', repl=' ', string=df.message[i])\n",
    "  # Converting the entire message into lower case\n",
    "  message = message.lower()\n",
    "   # Tokenizing the message by words\n",
    "  words = message.split()\n",
    "  # Removing the stop words\n",
    "  words = [word for word in words if word not in set(stopwords.words('english'))]\n",
    "  # Stemming the words\n",
    "  words = [ps.stem(word) for word in words]\n",
    "  # Joining the stemmed words\n",
    "  message = ' '.join(words)\n",
    "  # Building a corpus of messages\n",
    "  corpus.append(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2CuGj_ZM6fx6"
   },
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=3000)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "# Extracting dependent variable from the dataset\n",
    "y = pd.get_dummies(df['label'])\n",
    "y = y.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UZlwhXH58Sno"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Creating a pickle file for the CountVectorizer\n",
    "pickle.dump(cv, open('cv-transform.pkl', 'wb'))\n",
    "\n",
    "# Model Building\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0W-yoIM4FZff"
   },
   "outputs": [],
   "source": [
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB(alpha=0.3)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Creating a pickle file for the Multinomial Naive Bayes model\n",
    "filename = 'spam-sms-mnb-model.pkl'\n",
    "pickle.dump(classifier, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "eUuEdSZZFiiM"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F9nBB9QnGVxj",
    "outputId": "ef4230de-4e60-4ba4-b6a5-260ffa3c1c55"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9883408071748879"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oLMKlUnvGXVV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Spam SMS Classification",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
